---
title: "Longitudinal Binary Model"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
```
## Description of Data and Research Question

The data comes from user fitbit data and includes variables such as step count, time spent in different activity levels, and sleep. The user is identified by a unique ID and is repeatedly observed across time (in days). The research question is:

> to determine if the number of steps taken by a user is related to whether or not they slept more than 7 hours that night.

This is a longitudinal binary model because the response variable is binary (slept more than 7 hours or not) and the data is longitudinal (repeated observations across time). The prediction will be a probability of sleeping more than 7 hours.

We will include a random intercept for each individual to account for the individual's "baseline" probability of sleeping more than 7 hours.

### Data Exploration

```{r}
activity_data <- read.csv("data/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv")
sleep_data <- read.csv("data/Fitabase Data 4.12.16-5.12.16/sleepDay_merged.csv")

activity_data$ActivityDate <- as.Date(activity_data$ActivityDate, format = "%m/%d/%Y")

# calculate the sleep binary for each day
sleep_data$sleep_binary <- ifelse(sleep_data$TotalMinutesAsleep > 7*60, 1, 0)
# cast the SleepDay to date from datetime and name ActivityDate
# first from char to datetime
sleep_data$SleepDay <- as.POSIXct(sleep_data$SleepDay, format = "%m/%d/%Y %I:%M:%S %p")
# then from datetime to date
sleep_data$ActivityDate <- as.Date(sleep_data$SleepDay)

print(str(activity_data))
print(str(sleep_data))
```

We join the data. 

```{r}
# join the data on ActivityDate and Id
data <- merge(
    activity_data,
    sleep_data,
    by = c("Id", "ActivityDate")
)

print(str(data))
```

We display the count of observations for each idividual.

```{r}
data %>%
    group_by(Id) %>%
    summarise(n = n()) %>%
    arrange(desc(n))
```

We see that we do not have a uniform numer of observations for each individual. 

## Model Setup

### Random Intercept Model

Random intercept model. For subject iâ€™s jth observation, the random intercept model is defined as:

$$y_{ij} = \beta_0 + \beta_1x_{ij} + u_i + \epsilon_{ij}$$

where:

* $y_ij$ is the probability of sleeping more than 7 hours for subject i on day j
* $x_{ij}$ is the number of steps taken by subject i on day j
* $u_i$ is the random intercept for subject i
* $\epsilon_{ij} \sim N(0,\sigma_e^2)$ is the error term for subject i on day j

The matrix form of the model is:

$$Y = X\beta + ZU + \epsilon$$

where:

* $Y$ is the $n \times 1$ vector of responses
* $X$ is the $n \times p$ design matrix for the fixed effects
* $\beta$ is the $p \times 1$ vector of fixed effects
* $U$ is the $n \times 1$ vector of subject-specific intercepts
* $Z$ is the design matrix

$$Z = \begin{bmatrix} 
1 & 0 & \cdots & 0 \\
1 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1 \\
0 & 0 & \cdots & 1 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1 \\
\end{bmatrix}$$

### Probit Model

In our case, we follow Albert and Chib (1993, Note set II page 160) to represent bernoulli data as a latent variable model (equivalent to a probit model in the GLM frameworks).

We let $\zeta_i$ denote subjet i's latent variable:

$$y_i = \begin{cases} 1 & \text{if } \zeta_i > 0 \\ 0 & \text{if } \zeta_i \leq 0 \end{cases}$$

where $z_i \sim N(x_i^T\beta, 1)$ which is never observed. 

The likelihood for a single subject $i$ is then:

$$\mathbb{L}(y_i|\zeta_i,x_i,\beta)\propto 
\{1(\zeta_i>0)1(y_i=1) + 1(\zeta_i \leq 0)1(y_i=0)\} 
\exp{\left(
-\frac{1}{2}(\zeta_i - x_i^T\beta)^2
\right)}$$

In our case, unlike in Albert and Chib, we do not only have the $\beta$ parameter. We also have the random intercept parameter $u_i$ which we typically assume to have a prior $u_i \sim N(0, \sigma_u^2)$. Our choice of prior for $\beta$ determines the posterior distribution of $\beta$. We recall the bayesian relationship between the posterior and the likelihood:

$$p(\beta|y) \propto \mathbb{L}(y|\beta)\pi(\beta)\pi(u_i)$$.

### Priors

If we choose a non-informative prior $\pi(\beta)$ then we get the following posterior:

$$p(\beta|y) \propto \pi(\beta)\prod_{i=1}^n\mathbb{L}(y_i|\zeta_i,x_i,\beta)$$

We can also choose the prior for $\beta$ to conduct a ridge or lasso regression by choosing $\beta \sim N(0, \gamma)$ or $\beta \sim \text{Laplace}(0, \gamma)$ respectively.

# Derivation

Let $y_{ij}$ be the binary outcome for subject i (i = 1, ..., N) at time point j (j = 1, ..., T_i), where $y_{ij}$ = 1 if the subject slept for 7 hours or more and $y_ij$ = 0 otherwise. Let $x_{ij}$ be the step count variable for subject i at time point j.

Introduce a latent variable $\zeta_{ij}$ and a random intercept $u_i$, and define the model as follows:

$$\zeta_ij = x_{ij}^T\beta + u_i + \epsilon_{ij}$$

Here, $\beta$ is the regression coefficient associated with the step count variable, $u_i$ is the random intercept for subject i with $u_i \sim N(0, \sigma_u^2)$, and $\epsilon_{ij}$ is the independent and identically distributed error term with $\epsilon_{ij} \sim N(0, \sigma^2)$. $\sigma_u^2$ and $\sigma^2$ are the variances of the random intercept and error term, respectively.

The binary outcome, $y_{ij}$, is determined by the latent variable $\zeta_ij$ as follows:

$$y_{ij} = \begin{cases} 1 & \text{if } \zeta_{ij} > 0 \\ 0 & \text{if } \zeta_{ij} \leq 0 \end{cases}$$

To specify prior distributions For the Bayesian model, you need to specify prior distributions for $\beta$ and the variances $\sigma_u^2$ and $\sigma^2$. Assuming non-informative priors for the betas, we can consider the following priors:

* $p(\beta) \propto 1$ for a non-informative prior. 
* The random intercepts $u_i$ have a prior normal distribution: $u_i \sim N(0, \sigma^2)$, which implies $p(u_i|\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left(-\frac{u_i^2}{2\sigma^2}\right)$
* A prior distribution for the variance of the random intercepts: $p(\sigma^2)$. Wr can choose a non-informative prior for sigma squared, such as  $p(\sigma^2) \propto \frac{1}{\sigma^2}$.

The likelihood function for a single observation $y_ij$, given its latent variable $\zeta_{ij}$, step count $x_{ij}$, and coefficient $\beta$, is:

[comment]: <> ($$\mathbb{L}_{ij} = \begin{cases} N(\zeta_{ij}; x_{ij}^T\beta + u_i, \sigma^2)  & if y_{ij} = 1 \\ N(\zeta_{ij}; x_{ij}^T\beta + u_i, \sigma^2) & if y_{ij} = 0 \end{cases}$$)

$$\mathbb{L}(y_i|\zeta_i,x_i,\beta) \propto \left(1(\zeta_i>0)1(y_i=1) + 1(\zeta_i \leq 0)1(y_i=0)\right) \exp{\left(-\frac{1}{2}(\zeta_i - x_i^T\beta)^2\right)}$$

Considering all observations for subject i, the likelihood for subject i is:

$$\mathbb{L}(y|\zeta,X,\beta) = \prod_{i=1}^N \mathbb{L}(y_i|\zeta_i,x_i,\beta)$$

Combining the likelihood function and the prior distributions, we get the joint distribution as:

$$p(y, \zeta, \beta, u, \sigma^2 | X) \propto \mathbb{L}(y|\zeta,X,\beta) \times p(\beta) \times p(u|\sigma^2) \times p(\sigma^2)$$

Multiplying this out we get:

$$p(y, \zeta, \beta, u, \sigma^2 | X) \propto \prod_{i=1}^N \left(
1(\zeta_i>0)1(y_i=1) + 1(\zeta_i \leq 0)1(y_i=0)\right) \exp{\left(-\frac{1}{2}(\zeta_i - x_i^T\beta)^2\right)}
\times 1 \times \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left(-\frac{u_i^2}{2\sigma^2}\right)} \times \frac{1}{\sigma^2}$$

$$= \prod_{i=1}^N \left(
1(\zeta_i>0)1(y_i=1) + 1(\zeta_i \leq 0)1(y_i=0)\right) 
\exp{\left(-\frac{1}{2}(\zeta_i - x_i^T\beta)^2\right)}
\times {(\sigma^2)}^{-3/2} \exp{\left(-\frac{u_i^2}{2\sigma^2}\right)}$$

$$= \prod_{i=1}^N \left(
1(\zeta_i>0)1(y_i=1) + 1(\zeta_i \leq 0)1(y_i=0)\right) 
\exp{\left(-\frac{1}{2}(\zeta_i - x_i^T\beta)^2 -\frac{u_i^2}{2\sigma^2}\right)}
\times {(\sigma^2)}^{-3/2}$$

To obtain the posterior distribution $p(\beta, u, \sigma^2 | y, X)$, we integrate out $\zeta$ from the joint distribution:

$$p(\beta, u, \sigma^2 | y, X) \propto \int p(y, \zeta, \beta, u, \sigma^2 | X) , d\zeta$$

This integration does not have a closed-form solution, so we would typically rely on a sampling algorithm, such as Markov Chain Monte Carlo (MCMC) methods, to estimate the posterior distribution and obtain parameter estimates.

# Logistic Regression (Just for now)

```{r}
# logistic regression using base R
logit <- glm(sleep_binary ~ TotalSteps, data = data, family = binomial(link = "logit"))
summary(logit)
```

# Computing

```{r}
# Load required libraries
library(MASS) # for multivariate normal functions
library(coda) # for mcmc objects
library(mcmcplots) # for mcmcplot
```

```{r}
# Synthetic data generation for toy example
set.seed(123)
N <- 100
nGroups <- 10
p <- 3
true_beta <- c(2, -1, 3)
true_tau <- 1
X <- cbind(1, matrix(rnorm(N * (p - 1)), ncol = p - 1))
group <- factor(sample(1:nGroups, N, replace = TRUE))
random_effects <- rnorm(length(unique(group)), sd = sqrt(1 / true_tau))
y <- X %*% true_beta + random_effects[group] + rnorm(N, sd = 2)
```
```{r}
# Define prior parameters
prior_beta_mean <- rep(0, p)
prior_beta_cov <- diag(p)
prior_tau_shape <- 2
prior_tau_rate <- 1
```

```{r}
# Number of iterations and burn-in
niter <- 20000
burnin <- 10000
```

```{r}
# Initialize arrays to store samples
beta_samples <- matrix(NA, nrow = niter, ncol = p)
u_samples <- matrix(NA, nrow = niter, ncol = nGroups)
tau_samples <- rep(NA, niter)
```

```{r}
beta <- rep(0, p)
u <- rep(0, nGroups)
tau <- 1

for (i in 1:niter) {
  # Sample beta given tau, u, and the data
  X_residual <- y - u[group]
  post_beta_cov <- solve(solve(prior_beta_cov) + t(X) %*% X)
  post_beta_mean <- post_beta_cov %*% (solve(prior_beta_cov) %*% prior_beta_mean + t(X) %*% X_residual)
  beta <- mvrnorm(n = 1, mu = post_beta_mean, Sigma = post_beta_cov)

  # Update the model with new beta values
  X_residual <- y - X %*% beta

  # Sample U given beta, tau, and the data
  for (g in 1:nGroups) {
    this_group <- group == g
    group_size <- sum(this_group)
    post_u_mean <- sum(X_residual[this_group]) * tau / (tau * group_size + 1)
    post_u_var <- 1 / (tau * group_size + 1)
    u[g] <- rnorm(n = 1, mean = post_u_mean, sd = sqrt(post_u_var))
  }

  # Sample tau given beta, U, and the data
  group_residuals <- X_residual - u[group]
  post_tau_shape <- prior_tau_shape + N / 2
  post_tau_rate <- prior_tau_rate + sum(group_residuals^2) / 2
  tau <- rgamma(n = 1, shape = post_tau_shape, rate = post_tau_rate)

  # Store samples
  beta_samples[i, ] <- beta
  u_samples[i, ] <- u
  tau_samples[i] <- tau
}
```

```{r}
# hist for each distibution
hist(beta_samples)
#hist(u_samples)
hist(tau_samples)
```
```{r}
# Combine samples into an mcmc object
samples <- cbind(beta_samples, u_samples, tau_samples)
colnames(samples) <- c(paste0("beta_", 1:p), paste0("u_", 1:nGroups), "tau")
samples_mcmc <- as.mcmc(samples[-(1:burnin), ]) # Remove burn-in samples
```
```{r}
# Visualize convergence using mcmcplot
mcmcplot(samples_mcmc)
```